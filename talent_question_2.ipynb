{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Best problem provided by our talented students! \n",
    "### 1. When we train a neural network, we notice that the loss does not decrease in a few\n",
    "       starting epochs, what’s the reason for this? What is the strategy to improve?\n",
    "\n",
    "Please calculate X’s min-max normalizations, Z-score normalization, l2 normalization\n",
    "X = np.array([[ 1., -1., 2.],\n",
    "[ 2., 0., 0.],\n",
    "[ 0., 1., -1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The reasons for this could be</b>\n",
    "\n",
    "1. The learning is rate is low\n",
    "2. Regularization parameter is high\n",
    "3. Stuck at local minima\n",
    "\n",
    "<b>Strategy to improve?</b>\n",
    "\n",
    "1. Jitter the learning rate, i.e. change the learning rate for a few epochs\n",
    "2. change weight Initialization \n",
    "3. Increase or decrease learning rate\n",
    "4. Reduce Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "[2. 1. 2.]\n",
      "----------------------------\n",
      "Z-score\n",
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "----------------------------\n",
      "l2 normalization\n",
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# Min-max Normalization\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import sklearn \n",
    "\n",
    "X = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]])\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(X))\n",
    "MinMaxScaler()\n",
    "print(scaler.data_max_)\n",
    "\n",
    "# Z-score normalization\n",
    "print(\"----------------------------\")\n",
    "from scipy import stats\n",
    "\n",
    "print(\"Z-score\")\n",
    "print(stats.zscore(X))\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"l2 normalization\")\n",
    "print(sklearn.preprocessing.normalize(X, norm='l2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The problem is you are trying to train a deep learning model but only a small amount of data is available. Fortunately, there is a pre-trained neural network that was trained on a similar problem. Which one of the following methodologies is the best choice that can make use of this pre-trained network?\n",
    "\n",
    "A. Retrain the model from scratch for the new dataset\n",
    "\n",
    "B. Only fine-tune the last couple of layers of the pre-trained model with small learning\n",
    "rate \n",
    "\n",
    "C. Freeze all the layers except the last, and only retrain the last layer\n",
    "\n",
    "D. Assess on every layer how the pre-trained model performs and only select a few of them with\n",
    "respect to their performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Please provide at least three learning rate scheduling and briefly describe each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer : Common learning rate schedules are:\n",
    "1. constant, \n",
    "2. time-based decay, \n",
    "3. step decay and \n",
    "4. exponential decay.\n",
    "\n",
    "<b>Constant learning rate</b>: \n",
    "This is a default learning rate schedule in SGD optimizer in Keras. Both Momentum and decay rate are set to zero by default. It is tricky to choose the right learning rate. SGD optimizer also has an argument called nesterov which is set to false by default.\n",
    "\n",
    "<b>Time-Based Decay</b>: \n",
    "The mathematical form of time-based decay is lr = lr0/(1+kt) where lr, k are hyperparameters and t is the iteration number When the decay argument is specified, it will decrease the learning rate from the previous epoch by the given fixed amount.\n",
    "\n",
    "             lr *= (1. / (1. + self.decay * self.iterations))\n",
    "             \n",
    "             \n",
    "Momentum method helps the parameter vector to build up velocity in any direction with constant gradient descent so as to prevent oscillations. A typical choice of momentum is between 0.5 to 0.9. Nesterov momentum is another type of the momentum with stronger theoretical converge guarantees for convex functions and works slightly better than standard momentum.\n",
    "\n",
    "<b>Step Decay</b>: \n",
    "Step decay schedule drops the learning rate by a factor every few epochs. A typical way of calculating factor is to drop the learning rate by half every 10 epochs.\n",
    "The mathematical form of step decay is :\n",
    "                lr = lr0 * drop^floor(epoch / epochs_drop)\n",
    "  \n",
    "<b>Exponential Decay</b>:\n",
    "Another common schedule is exponential decay. It has the mathematical form\n",
    "\n",
    "              lr = lr0 * e^(−kt)\n",
    "where lr, k are hyperparameters and t is the iteration number. Similarly, we can implement this by defining exponential decay function,but the only difference is we define a different custom decay function.\n",
    "\n",
    "def exp_decay(epoch):\n",
    "   initial_lrate = 0.1\n",
    "   k = 0.1\n",
    "   lrate = initial_lrate * exp(-k*t)\n",
    "   return lrate\n",
    "lrate = LearningRateScheduler(exp_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
