{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:1\n",
    "There are many activations utilized in DL, e.g., sigmoid, ReLU, SeLU, etc. Can you compare their pros and cons? Please also address proper application contexts for each activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer.1:\n",
    "### 1.Sigmoid Activation Function: \n",
    "The sigmoid function is a logistic function, which means that, whatever you input, you get an output ranging between 0 and 1. That is, every neuron, node or activation that you input, will be scaled to a value between 0 and 1.\n",
    "                                    sigmoid(x)=œÉ=1/1+e‚àíx\n",
    "\n",
    "### Pros: \n",
    "It is nonlinear in nature. Combinations of this function are also nonlinear.\n",
    "1. It will give an analog activation unlike step function.\n",
    "2. It has a smooth gradient too.\n",
    "3. It‚Äôs good for a classifier.\n",
    "4. The output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function. So we have our activations bound in a range. \n",
    "### Cons:\n",
    "1. Towards either end of the sigmoid function, the Y values tend to respond very less to changes in X.\n",
    "2. It gives rise to a problem of ‚Äúvanishing gradients‚Äù.\n",
    "3. Its output isn‚Äôt zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.\n",
    "4. Sigmoids saturate and kill gradients.\n",
    "5. The network refuses to learn further or is drastically slow.\n",
    "\n",
    "### Application:\n",
    "1. If your output is for binary classification then, sigmoid function is very natural choice for output layer.\n",
    "Usually used in output layer of a binary classification, where result is either 0 or 1, as value for sigmoid function lies between 0 and 1 only so, result can be predicted easily to be 1 if value is greater than 0.5 and 0 otherwise.\n",
    "\n",
    "2. Examples of the application of the logistic S-curve to the response of crop yield (wheat) to both the soil salinity and depth to water table in the soil are shown in logistic function for example in agriculture for modelling modeling crop response.\n",
    "\n",
    "4. In artificial neural networks, sometimes non-smooth functions are used instead for efficiency; these are known as hard sigmoids.\n",
    "\n",
    "5. In audio signal processing, sigmoid functions are used as waveshaper transfer functions to emulate the sound of analog circuitry clipping.[4]\n",
    "\n",
    "6. In biochemistry and pharmacology, the Hill equation and Hill-Langmuir equation are sigmoid functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.ReLU Activation Function: \n",
    "It computes the function ùëì(ùë•)=max(0,ùë•)f(x)=max(0,x). In other words, the activation is simply thresholded at zero.\n",
    "### Pros:\n",
    "1. Less time and space complexity, because of sparsity, and compared to the sigmoid, it does not evolve the exponential operation, which are more costly. So, ReLU is computationally efficient\n",
    "2. Avoids the vanishing gradient problem.\n",
    "3. It was found to greatly accelerate the convergence of stochastic gradient descent compared to the sigmoid/tanh functions\n",
    "4. ReLU can be implemented by simply thresholding a matrix of activations at zero.\n",
    "5. ReLU has a derivative function and allows for backpropagation.\n",
    "\n",
    "### Cons:\n",
    "1. The Dying ReLU problem‚Äîwhen inputs approach zero, or are negative, the gradient of the function becomes zero, the network cannot perform backpropagation and cannot learn.\n",
    "2. Unfortunately, ReLU units can be fragile during training and can \"die\". A large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on.\n",
    "3. In practice, we could find that as much as 40% of your network can be \"dead\" (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. \n",
    "4. ReLUs does not avoid the exploding gradient problem.\n",
    "\n",
    "\n",
    "### Application:\n",
    "\n",
    "1. It‚Äôs also helpful if you wish to apply a ‚Äúfilter‚Äù to partially keep a certain value (like in an LSTM‚Äôs forget gate).\n",
    "2. Biological plausibility is  One-sided for ReLU whereas biological plausibility is antisymmetry compared to the tanh.\n",
    "3. Sparse activation: For example, in a randomly initialized network, only about 50% of hidden units are activated (have a non-zero output).\n",
    "4. Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.ELU Activation Function: \n",
    "                     {x                           if x>0\n",
    "                     {Œ±(e with power(x) ‚àí1),       if x<0\n",
    "                     \n",
    "### Pros:\n",
    "1. Avoids the dead relu problem.\n",
    "2. Produces negative outputs, which helps the network nudge weights and biases in the right directions.\n",
    "3. Produce activations instead of letting them be zero, when calculating the gradient.\n",
    "\n",
    "\n",
    "### Cons:\n",
    "1. Introduces longer computation time, because of the exponential operation included\n",
    "2. Does not avoid the exploding gradient problem\n",
    "3. The neural network does not learn the alpha value\n",
    "\n",
    "### Application:\n",
    "ELU using a ConvNet that is trained on the MNIST dataset, the results suggest that ELU benefits when we train for many epochs, possibly with deeper networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.SELU Activation Function:\n",
    "The equation for it looks like this:\n",
    "                 SELU(x)=Œª {x              if x>0\n",
    "                           {Œ±ex‚àíŒ±          if x‚â§0\n",
    "                           \n",
    "#### Pros:\n",
    "1. Internal normalization is faster than external normalization, which means the network converges faster.\n",
    "2. Vanishing and exploding gradient problem is impossible, shown by their theorems 2 & 3 in the appendix.\n",
    "3. ReLU function is the most widely used function and performs better than other activation functions in most of the cases. \n",
    "\n",
    "### Cons\n",
    "\n",
    "1. Relatively new activation function ‚Äì needs more papers on architectures such as CNNs and RNNs, where it is comparatively explored\n",
    "2. ReLU function has to be used only in the hidden layers and not in the outer layer\n",
    "\n",
    "### Application:\n",
    "1. For SNNs (Self Normalizing Networks)to work, they need two things, a custom weight initialization method and the SELU activation function.\n",
    "   SNNs are a way to instead use external normalization techniques (like batch norm), the normalization occurs inside the activation function.\n",
    "   To make it clear, instead of normalizing the output of the activation function ‚Äî the activation function suggested (SELU ‚Äî scaled exponential linear units) outputs normalized values.SNNs, using the specified initialization and the SELU activation function, does on the MNIST and CIFAR-10 datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.Tanh Activation Function:\n",
    "tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n",
    "\n",
    "#### Pros:\n",
    "1. The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
    "2. The function is differentiable.\n",
    "3. The function is monotonic while its derivative is not monotonic.\n",
    "4. The tanh function is mainly used classification between two classes.\n",
    "5. Both tanh and logistic sigmoid activation functions are used in feed-forward nets.\n",
    "\n",
    "### Cons:\n",
    "1. A general problem with both the sigmoid and tanh functions is that they saturate. This means that large values snap to 1.0 and small values snap to -1 or 0 for tanh and sigmoid respectively. Further, the functions are only really sensitive to changes around their mid-point of their input, such as 0.5 for sigmoid and 0.0 for tanh.\n",
    "\n",
    "2. The limited sensitivity and saturation of the function happen regardless of whether the summed activation from the node provided as input contains useful information or not. Once saturated, it becomes challenging for the learning algorithm to continue to adapt the weights to improve the performance of the model.\n",
    "\n",
    "#### Application: \n",
    "1. Both tanh and logistic sigmoid activation functions are used in feed-forward nets.\n",
    "2. The tanh function is mainly used classification between two classes.\n",
    "3. Usually used in hidden layers of a neural network as it‚Äôs values lies between -1 to 1 hence the mean for the hidden layer comes out be 0 or very close to it, hence helps in centering the data by bringing mean close to 0. This makes learning for the next layer much easier.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:2)How to avoid overfitting at DNN model? Please discuss at least three ways and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:2)\n",
    "#### 1. L1 and L2 Regularization:\n",
    "The biggest reasons for regularization are 1) to avoid overfitting by not generating high coefficients for predictors that are sparse. 2) to stabilize the estimates especially when there's collinearity in the data.\n",
    "2. A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\n",
    "3. Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds ‚Äúabsolute value of magnitude‚Äù of coefficient as penalty term to the loss function whereas Ridge regression adds ‚Äúsquared magnitude‚Äù of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.\n",
    "4. For L1 sparsity wll be like( 1,0,0,1,1) whereas for L2 it is( 0.5, 0.3 ,-0.2, 0.1)\n",
    "5. L1 is good for feature selection whereas L2 normally better for training models.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Drop out: \n",
    "1. This idea is actually very simple - every unit of our neural network (except those belonging to the output layer) is given the probability p of being temporarily ignored in calculations. Hyper parameter p is called dropout rate and very often its default value is set to 0.5. Sometimes one part of the network have very large weights and it ends up dominating all the training,and other part of the network doesn't really play much of a role. So, we turn this part off and let the rest of the network train.\n",
    "\n",
    "#### 3. Early stopping:\n",
    "1. It is very convenient to sample our model every few iterations and check how well it works with our validation set. Every model that performs better than all the previous models is saved. We also set a limit, i.e. the maximum number of iterations during which no progress will be recorded. When this value is exceeded, the learning is stopped. Although early stopping allows for a significant improvement in the performance of our model, in practice, its application greatly complicates the process of optimization of our model. \n",
    "\n",
    "#### 4. lambda factor (regularization rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:3)How to use Data Normalization to improve DNN model training and performance? Please discuss at least two methods and compare them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:3)\n",
    "Normalization is an approach which is applied during the preparation of data in order to change the values of numeric columns in a dataset to use a common scale when the features in the data have different ranges. The following methods are different approaches to normalize data:\n",
    "1. Batch Normalization\n",
    "2. Weight Normalization\n",
    "3. Layer Normalization\n",
    "4. Group Normalization\n",
    "5. Instance Normalization\n",
    "\n",
    "\n",
    "#### Batch Normalization: \n",
    "1. Batch normalization is a general technique that can be used to normalize the inputs to a layer.\n",
    "2. It can be used with most network types, such as Multilayer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks. It may be more appropriate after the activation function if for s-shaped functions like the hyperbolic tangent and logistic function.\n",
    "3. It may be appropriate before the activation function for activations that may result in non-Gaussian distributions like the rectified linear activation function, the modern default for most network types.\n",
    "4. Using batch normalization makes the network more stable during training.This may require the use of much larger than normal learning rates, that in turn may further speed up the learning process.\n",
    "5. In a batch-normalized model, we have been able to achieve a training speedup from higher learning rates, with no ill side effects\n",
    "6. The faster training also means that the decay rate used for the learning rate may be increased.\n",
    "7. Batch normalization offers some regularization effect, reducing generalization error, perhaps no longer requiring the use of dropout for regularization.\n",
    "8. It enables faster and stable training of deep neural networks by stabilising the distributions of layer inputs during the training phase. This approach is mainly related to internal covariate shift (ICS) where internal covariate shift means the change in the distribution of layer inputs caused when the preceding layers are updated. In order to improve the training in a model, it is important to reduce the internal co-variant shift.\n",
    "\n",
    "9. The advantages of batch normalization are mentioned below:\n",
    "Batch normalization reduces the internal covariate shift (ICS) and accelerates the training of a deep neural network.This approach reduces the dependence of gradients on the scale of the parameters or of their initial values which result in higher learning rates without the risk of divergence.Batch Normalisation makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes.\n",
    "\n",
    "\n",
    "#### Weight Normalization:\n",
    "1. It is a method developed by Open AI that, instead of normalizing the mini-batch, normalizes the weights of the layer.\n",
    "2. Weight normalization is a process of reparameterization of the weight vectors in a deep neural network which works by decoupling the length of those weight vectors from their direction. In simple terms, we can define weight normalization as a method for improving the optimisability of the weights of a neural network model.\n",
    "3.  By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent.\n",
    "4. They then propose to reparameterize each weight vector ùê∞ in terms of a parameter vector ùêØ and a scalar parameter ùëî and to perform stochastic gradient descent with respect to those parameters instead.\n",
    "\n",
    "                            ùê∞=g*v /‚ÄñùêØ‚Äñ\n",
    "   \n",
    "   where ùêØ is a ùëò-dimensional vector, ùëî is a scalar, and ‚ÄñùêØ‚Äñ denotes the Euclidean norm of ùêØ. They call this reparameterizaton weight normalization.\n",
    "   \n",
    "\n",
    "5. Weight normalization improves the conditioning of the optimisation problem as well as speed up the convergence of stochastic gradient descent.\n",
    "6. It can be applied successfully to recurrent models such as LSTMs as well as in deep reinforcement learning or generative models\n",
    "7. The experimental results of the paper show that weight normalization combined with mean-only batch normalization achieves the best results on CIFAR-10, an image classification dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between batch normalization and weight normalization:\n",
    "### Batch Normalization:\n",
    "## Pros:\n",
    "1. Stable if the batch size is large\n",
    "2.  Robust (in train) to the scale & shift of input data\n",
    "3. Robust to the scale of weight vector\n",
    "4. Scale of update decreases while training\n",
    "\n",
    "### Cons:\n",
    "1.  Not good for online learning\n",
    "2.  Not good for RNN, LSTM\n",
    "3. Different calculation between train and test\n",
    "\n",
    "### Weight Normalization:\n",
    "    \n",
    "### Pros\n",
    "1. Smaller calculation cost on CNN\n",
    "2. Well-considered about weight initialization\n",
    "3. Implementation is easy\n",
    "4. Robust to the scale of weight vector\n",
    "### Cons:\n",
    "1. Compared with the others, might be unstable on training\n",
    "2. High dependence to input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Best problem provided by our talented students! \n",
    "##### 1. When we train a neural network, we notice that the loss does not decrease in a few starting epochs, what‚Äôs the reason for this? What is the strategy to improve?\n",
    "\n",
    "#Please calculate X‚Äôs min-max normalizations, Z-score normalization, l2 normalization\n",
    "import numpy as np\n",
    "X = np.array([[ 1., -1., 2.],\n",
    "[ 2., 0., 0.],\n",
    "[ 0., 1., -1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The reasons for this could be</b>\n",
    "\n",
    "1. The learning is rate is low\n",
    "2. Regularization parameter is high\n",
    "3. Stuck at local minima\n",
    "\n",
    "<b>Strategy to improve?</b>\n",
    "\n",
    "1. Jitter the learning rate, i.e. change the learning rate for a few epochs\n",
    "2. Change weight Initialization \n",
    "3. Increase or decrease learning rate\n",
    "4. Reduce Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "[2. 1. 2.]\n",
      "[ 0. -1. -1.]\n",
      "----------------------------\n",
      "Z-score\n",
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "----------------------------\n",
      "l2 normalization\n",
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# Min-max Normalization\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import sklearn \n",
    "\n",
    "X = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]])\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(X))\n",
    "MinMaxScaler()\n",
    "print(scaler.data_max_)\n",
    "print(scaler.data_min_)\n",
    "\n",
    "# Z-score normalization\n",
    "print(\"----------------------------\")\n",
    "from scipy import stats\n",
    "\n",
    "print(\"Z-score\")\n",
    "print(stats.zscore(X))\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"l2 normalization\")\n",
    "print(sklearn.preprocessing.normalize(X, norm='l2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The problem is you are trying to train a deep learning model but only a small amount of data is available. Fortunately, there is a pre-trained neural network that was trained on a similar problem. Which one of the following methodologies is the best choice that can make use of this pre-trained network?\n",
    "\n",
    "A. Retrain the model from scratch for the new dataset\n",
    "\n",
    "B. Only fine-tune the last couple of layers of the pre-trained model with small learning\n",
    "rate \n",
    "\n",
    "C. Freeze all the layers except the last, and only retrain the last layer\n",
    "\n",
    "D. Assess on every layer how the pre-trained model performs and only select a few of them with\n",
    "respect to their performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : C (Freeze all the layers except the last, and only retrain the last layer). \n",
    "If the dataset is mostly similar, the best method would be to train only the last layer, as previous all layers work as feature extractors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Please provide at least three learning rate scheduling and briefly describe each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer : Common learning rate schedules are:\n",
    "1. constant, \n",
    "2. time-based decay, \n",
    "3. step decay and \n",
    "4. exponential decay.\n",
    "\n",
    "<b>Constant learning rate</b>: \n",
    "This is a default learning rate schedule in SGD optimizer in Keras. Both Momentum and decay rate are set to zero by default. It is tricky to choose the right learning rate. SGD optimizer also has an argument called nesterov which is set to false by default.\n",
    "\n",
    "<b>Time-Based Decay</b>: \n",
    "The mathematical form of time-based decay is lr = lr0/(1+kt) where lr, k are hyperparameters and t is the iteration number When the decay argument is specified, it will decrease the learning rate from the previous epoch by the given fixed amount.\n",
    "\n",
    "             lr *= (1. / (1. + self.decay * self.iterations))\n",
    "             \n",
    "             \n",
    "Momentum method helps the parameter vector to build up velocity in any direction with constant gradient descent so as to prevent oscillations. A typical choice of momentum is between 0.5 to 0.9. Nesterov momentum is another type of the momentum with stronger theoretical converge guarantees for convex functions and works slightly better than standard momentum.\n",
    "\n",
    "<b>Step Decay</b>: \n",
    "Step decay schedule drops the learning rate by a factor every few epochs. A typical way of calculating factor is to drop the learning rate by half every 10 epochs.\n",
    "The mathematical form of step decay is :\n",
    "                lr = lr0 * drop^floor(epoch / epochs_drop)\n",
    "  \n",
    "<b>Exponential Decay</b>:\n",
    "Another common schedule is exponential decay. It has the mathematical form\n",
    "\n",
    "              lr = lr0 * e^(‚àíkt)\n",
    "where lr, k are hyperparameters and t is the iteration number. Similarly, we can implement this by defining exponential decay function,but the only difference is we define a different custom decay function.\n",
    "The python function for exponential decay as follows:\n",
    "\n",
    "def exp_decay(epoch):\n",
    "   initial_lrate = 0.1\n",
    "   k = 0.1\n",
    "   lrate = initial_lrate * exp(-k*t)\n",
    "   return lrate\n",
    "lrate = LearningRateScheduler(exp_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4( California Housing) and Answer 6( Fashion MNIST) are in separate google colab notebooks with .ipynb file format. I compressed  all 3 files in a single zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
